{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bca4ffb-53e3-4166-b293-2f1d0046f8a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d128f138-f809-4001-a8cd-a9b15b38353b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Using Unity Catalog for the Raw Layer (instead of `mnt`)\n",
    "\n",
    "Unity Catalog replaces the need for DBFS mounts by managing secure access to cloud storage through Storage Credentials and External Locations.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. **Create an Azure Databricks Access Connector**  \n",
    "   - This identity will be used by Unity Catalog to access ADLS Gen2.\n",
    "\n",
    "2. **Assign RBAC permissions on the Storage Account**  \n",
    "   - Give the Access Connector the required roles (e.g., *Storage Blob Data Contributor*) on the target container or folder.\n",
    "\n",
    "3. **Create a Storage Credential in Unity Catalog**  \n",
    "   - This credential references the Access Connector and is used to authenticate to ADLS.\n",
    "\n",
    "4. **Create an External Location**  \n",
    "   - Point it to the specific ADLS Gen2 path, e.g.:  \n",
    "     `abfss://raw@<storage-account>.dfs.core.windows.net/`\n",
    "\n",
    "5. **Create a Catalog**  \n",
    "   - This organizes schemas and tables under Unity Catalog governance.\n",
    "\n",
    "6. **Create a Schema (e.g., `raw`)**  \n",
    "   - This schema will contain your external tables.\n",
    "\n",
    "7. **Create External Tables referencing the External Location**  \n",
    "   - Use SQL like:  \n",
    "     ```sql\n",
    "     CREATE TABLE raw.my_table\n",
    "     USING parquet\n",
    "     LOCATION 'abfss://raw@<storage-account>.dfs.core.windows.net/my_table/';\n",
    "     ```\n",
    "   - Spark DataFrames can also write directly to these paths using Unity Catalog governance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c38765f-6fce-4beb-9af4-c71359ae93f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "8.(Nico) Creating Raw Tables for Unity Catalog",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
